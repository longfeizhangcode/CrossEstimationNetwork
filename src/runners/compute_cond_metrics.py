# Copyright (c) 2024 Longfei Zhang
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.
# =============================================================================
# Author: Longfei Zhang
# Affil.: Collaborative Innovation Center of Assessment for
#         Basic Education Quality, Beijing Normal University
# E-mail: zhanglf@mail.bnu.edu.cn
# =============================================================================
"""Compute the recovery metrics based on the range of true values of the
parameters.

The recovery metrics of the estimates generated by the CEN methods may
vary depending on the range of their true values. For instance, the Mean
Squared Error (MSE) of 'z' estimates, where their true values are within
the range (-inf, 2), could differ from that where the true values are
within the range (0, 1).
"""

import itertools
import math
import os
import sys

import numpy as np
import pandas as pd
from scipy import stats
from sklearn import metrics

# Add the "src" directory to sys.path, which makes sure the interpreter
# accesses the required modules appropriately.
current = os.path.dirname(os.path.realpath(__file__))
parent = os.path.dirname(current)
sys.path.append(parent)

from models.cen import CEN
from utils import utils


def get_recovery_metrics(
    estimates_mat,
    cut_points,
):
    """Compute the recovery metrics of the estimates for different
    groups in the scale of true values for the parameters 'z', 'a' or
    'b'.

    Args:
        estimates_mat (numpy.ndarray): Estimates of the parameters 'z',
            'a' or 'b' produced by all the CEN methods.
        cut_points (list): The cut points to divide the groups in the
            scale of the true values.

    Return:
        numpy.ndarray: The recovery metrics related to all the CEN
        versions within a single repetition: the first dimension links
        to the groups, the second to the CEN versions, and the third to
        the metric types.
    """

    def sort_estimates_mat(estimates_mat, cut_points):
        """Sort the estimates matrix into several groups indicated by
        'cut_points'.

        Return:
            list: the sorted matrices of parameter estimates. The length
            of this list equals to the number of groups, and each
            element in the list is a 2D 'numpy.ndarray' which records
            the true values and estimates obtained by all the CEN
            methods in that group.
        """
        true_values = estimates_mat[0,]
        boundaries = np.insert(cut_points, 0, -math.inf)
        boundaries = np.insert(boundaries, len(boundaries), math.inf)
        n_boundaries = len(boundaries)

        estimates_mat_sorted = []

        j = 0
        for j in range(n_boundaries - 1):
            indices = (true_values >= boundaries[j]) & (true_values < boundaries[j + 1])
            estimates_mat_sorted.append(estimates_mat[:, indices])

        return estimates_mat_sorted

    n_groups = len(cut_points) + 1
    estimates_mat_sorted = sort_estimates_mat(estimates_mat, cut_points)

    metrics = np.full(
        shape=(n_groups, n_CEN_versions + 1, n_metrics),
        fill_value=np.nan,
    )

    for i in range(n_groups):
        current_group = estimates_mat_sorted[i]
        for j in range(n_CEN_versions + 1):
            if len(current_group[0, :]):
                metrics[i, j, :] = compute_distance(
                    current_group[0, :], current_group[j + 1, :]
                )

    return metrics


def compute_distance(param_true, param_est):
    """Compute the distance between the actual and estimated values
    pertaining to the given parameter, which indicates the recovery of
    the estimates.

    Args:
        param_true (numpy.ndarray): The ground truth values of the
            parameter.
        param_est (numpy.ndarray): The estimates of the parameter.

    Returns:
        numpy.ndarray: An array containing bias (Bias), mean absolute
        error (MAE), mean squared error (MSE), and root mean squared
        error (RMSE) between the actual and estimated values of the
        parameter, with type 'float'.
    """
    bias = np.mean(param_true - param_est)
    mae = metrics.mean_absolute_error(param_true, param_est)
    mse = metrics.mean_squared_error(param_true, param_est)
    rmse = metrics.mean_squared_error(param_true, param_est) ** 0.5

    return np.array([bias, mae, mse, rmse])


def save_cond_metrics_summary(
    path,
    cond_metrics_mean,
    cond_metrics_sem,
    which_param,
):
    """Save the summary of conditional metrics ('Mean' or 'SEM')to the
    specified directory.

    Args:
        path (str): Path to the directory for saving the summary results.
        cond_metrics_mean (numpy.ndarray): 3D array including the mean
            of metrics of all the CEN versions in all groups.
        cond_metrics_sem (numpy.ndarray): 3D array including the SEM of
            metrics of all the CEN versions in all groups.
        which_param (str): Which parameter are we gonna save? Specify
            from 'z', 'a', 'b'.
    """

    for idx_group in range(len(cond_metrics_mean)):
        adorn_metrics(cond_metrics_mean[idx_group]).to_csv(
            os.path.join(path, f"{which_param}_mean_group{idx_group}.csv")
        )
        adorn_metrics(cond_metrics_sem[idx_group]).to_csv(
            os.path.join(path, f"{which_param}_sem_group{idx_group}.csv")
        )


def save_cond_metrics(
    path,
    cond_metrics,
    which_param,
):
    """Save the conditional metrics to the specified directory.

    Args:
        path (str): Path to the directory for saving the results.
        cond_metrics (numpy.ndarray): 3D array including the metrics of
            all the CEN versions in all groups within a repetition.
        which_param (str): Which parameter are we gonna save? Specify
            from 'z', 'a', 'b'.
    """

    for idx_group in range(len(cond_metrics)):
        adorn_metrics(cond_metrics[idx_group]).to_csv(
            os.path.join(path, f"{which_param}_group{idx_group}.csv")
        )


def adorn_metrics(metrics_mat):
    """Improve the appearance of the recovery metrics."""
    metrics_mat = pd.DataFrame(metrics_mat)
    metrics_mat.index = CEN_versions_names
    metrics_mat.columns = ["Bias", "MAE", "MSE", "RMSE"]

    return metrics_mat


# ======================================================================
# Settings for this study
# ======================================================================

# Settings for the test.
n_persons = 1000
n_items = 90
n_reps = 100

# Settings for saving results.
n_metrics = 4

# Settings for building CEN.
net_depth_levels = [1, 3]
linear_nonlinear = [True, False]

# Cut points defining ranges for the true values of the specific
# parameter. For example, there are five cut points relevant to 'z', all
# the true values of 'z' are thus divided into six groups, the leftmost
# is negative infinity to -2 and the rightmost is 2 to positive infinity.
z_cut_points = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])
a_cut_points = np.array([0.8, 1.4])
b_cut_points = np.array([-1.5, 0.0, 1.5])

n_CEN_versions = len(net_depth_levels) * len(linear_nonlinear)

# Get the list including the names of all CEN versions.
CEN_versions_names = []
for net_depth, linear in itertools.product(net_depth_levels, linear_nonlinear):
    if linear:
        linear = ""
    else:
        linear = "N"
    CEN_versions_names.append(f"CEN_{net_depth}H_{linear}L")
CEN_versions_names.append("EM/EAP")

# Container for saving results in repetitions.
z_cond_metrics_reps = np.zeros(
    (
        n_reps,
        len(z_cut_points) + 1,
        n_CEN_versions + 1,
        n_metrics,
    )
)
a_cond_metrics_reps = np.zeros(
    (
        n_reps,
        len(a_cut_points) + 1,
        n_CEN_versions + 1,
        n_metrics,
    )
)
b_cond_metrics_reps = np.zeros(
    (
        n_reps,
        len(b_cut_points) + 1,
        n_CEN_versions + 1,
        n_metrics,
    )
)


# ======================================================================
# The main process of this study
# ======================================================================

for rep in range(n_reps):
    # Load the data in Simulation Study 1.
    path_EvalTrain = (
        f"./data/n_persons_{n_persons}/n_items_{n_items}/rep_{rep}/EvalTrain"
    )
    res_mat = np.loadtxt(
        os.path.join(path_EvalTrain, "res_mat.csv"),
        dtype="int",
        delimiter=",",
    )
    z_true = np.loadtxt(
        os.path.join(path_EvalTrain, "z_true.csv"),
        dtype="float",
        delimiter=",",
    )
    a_true = np.loadtxt(
        os.path.join(path_EvalTrain, "a_true.csv"),
        dtype="float",
        delimiter=",",
    )
    b_true = np.loadtxt(
        os.path.join(path_EvalTrain, "b_true.csv"),
        dtype="float",
        delimiter=",",
    )

    # Each '*_est_all_versions' variable is intended to save the
    # estimates obtained by all the CEN versions for 'z', 'a' or 'b'
    # within a repetition.
    z_est_all_versions = z_true
    a_est_all_versions = a_true
    b_est_all_versions = b_true

    for net_depth, linear in itertools.product(
        net_depth_levels,
        linear_nonlinear,
    ):
        # Initialize an instance of CEN, the network weights saved in
        # Simulation Study 1 wold be loaded onto it later.
        cen = CEN(
            inp_size_person_net=n_items,
            inp_size_item_net=n_persons,
            person_net_depth=net_depth,
            item_net_depth=net_depth,
            linear=linear,
            show_model_layout=False,
        )

        # Restore the weights of the trained PN and IN in Simulation Study 1.
        path_models = f"./models/simulation_study1/n_persons_{n_persons}/n_items_{n_items}/rep_{rep}/net_depth_{net_depth}/linear_{linear}"
        cen.person_net.load_weights(os.path.join(path_models, "person_net/person_net"))
        cen.item_net.load_weights(os.path.join(path_models, "item_net/item_net"))

        z_est = cen.param_est(param="z", res_mat=res_mat)
        a_est = cen.param_est(param="a", res_mat=res_mat)
        b_est = cen.param_est(param="b", res_mat=res_mat)

        z_est_all_versions = np.vstack((z_est_all_versions, z_est))
        a_est_all_versions = np.vstack((a_est_all_versions, a_est))
        b_est_all_versions = np.vstack((b_est_all_versions, b_est))

    # Compute conditional recovery metrics for the three types of
    # parameters within a repetition.

    z_est_r = np.loadtxt(
        f"./n_items_90/rep_{rep}/z_est_r.csv",
        dtype="float",
        delimiter=",",
    )
    a_est_r = np.loadtxt(
        f"./n_items_90/rep_{rep}/a_est_r.csv",
        dtype="float",
        delimiter=",",
    )
    b_est_r = np.loadtxt(
        f"./n_items_90/rep_{rep}/b_est_r.csv",
        dtype="float",
        delimiter=",",
    )

    z_est_all_versions = np.vstack((z_est_all_versions, z_est_r))
    a_est_all_versions = np.vstack((a_est_all_versions, a_est_r))
    b_est_all_versions = np.vstack((b_est_all_versions, b_est_r))

    z_cond_metrics = get_recovery_metrics(
        estimates_mat=z_est_all_versions,
        cut_points=z_cut_points,
    )
    a_cond_metrics = get_recovery_metrics(
        estimates_mat=a_est_all_versions,
        cut_points=a_cut_points,
    )
    b_cond_metrics = get_recovery_metrics(
        estimates_mat=b_est_all_versions,
        cut_points=b_cut_points,
    )

    path_to_rep = (
        f"./results/cond_metrics/n_persons_{n_persons}/n_items_{n_items}/rep_{rep}"
    )
    utils.make_dir(path_to_rep)

    save_cond_metrics(
        path_to_rep,
        z_cond_metrics,
        "z",
    )
    save_cond_metrics(
        path_to_rep,
        a_cond_metrics,
        "a",
    )
    save_cond_metrics(
        path_to_rep,
        b_cond_metrics,
        "b",
    )

    z_cond_metrics_reps[rep,] = z_cond_metrics
    a_cond_metrics_reps[rep,] = a_cond_metrics
    b_cond_metrics_reps[rep,] = b_cond_metrics

# Calculate the Mean and SEM across all repetitions.
z_cond_metrics_mean = np.nanmean(z_cond_metrics_reps, 0)
a_cond_metrics_mean = np.nanmean(a_cond_metrics_reps, 0)
b_cond_metrics_mean = np.nanmean(b_cond_metrics_reps, 0)

z_cond_metrics_sem = stats.sem(z_cond_metrics_reps, 0, nan_policy="omit")
a_cond_metrics_sem = stats.sem(a_cond_metrics_reps, 0, nan_policy="omit")
b_cond_metrics_sem = stats.sem(b_cond_metrics_reps, 0, nan_policy="omit")

path_to_summary = (
    f"./results_summary/cond_metrics/n_persons_{n_persons}/n_items_{n_items}/summary"
)
utils.make_dir(path_to_summary)

save_cond_metrics_summary(
    path_to_summary,
    z_cond_metrics_mean,
    z_cond_metrics_sem,
    "z",
)
save_cond_metrics_summary(
    path_to_summary,
    a_cond_metrics_mean,
    a_cond_metrics_sem,
    "a",
)
save_cond_metrics_summary(
    path_to_summary,
    b_cond_metrics_mean,
    b_cond_metrics_sem,
    "b",
)
